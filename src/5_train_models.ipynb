{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1df04bc-cd6c-4abf-9e6f-e7c1da2be89f",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbeea2cf-18aa-476e-a4e6-ad0133952d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "INPUT_DIR = Path(\"../data\")\n",
    "OUTPUT_DIR = Path(\"../models\")\n",
    "\n",
    "SAMPLES_FN = \"{n_fold}_samples_train.pq\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe796e8-48e6-4860-9dd9-263afa2baf81",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96819d5-9c27-4258-955f-9d05afde52a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a414c655-19d9-4c63-9fc3-0107d4c336c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "TARGET_COLUMN = 'class'\n",
    "SPATIAL_CROSS_VALIDATION_COLUMN = 'tile_id'\n",
    "\n",
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "covariates = [f'B{n:02}' for n in range(1, 65)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f99ac300-f5a3-46a4-bec0-65b718c93e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def target_ovo(samples: pd.DataFrame, class_name: str, class_a: list[int], class_b: list[int]):\n",
    "    remap_dict = {}\n",
    "    \n",
    "    remap_dict.update({val: 0.0 for val in class_a})\n",
    "    remap_dict.update({val: 1.0 for val in class_b})\n",
    "    \n",
    "    samples[class_name] = samples[TARGET_COLUMN].map(remap_dict)\n",
    "\n",
    "\n",
    "def create_ovo_class(samples: pd.DataFrame, class_name: list[str], class_values: list[tuple[list[int], list[int]]]):\n",
    "    class_data = dict(zip(class_name, class_values))\n",
    "    \n",
    "    for class_key in class_data:\n",
    "        value_a = class_data[class_key][0]\n",
    "        value_b = class_data[class_key][1]\n",
    "        \n",
    "        target_ovo(samples, class_key, value_a, value_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260a915",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c61a1ee-96a1-4942-b9d7-daf72ee4c1a5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_estimator():\n",
    "    return RandomForestClassifier(n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def random_forest(samples: pd.DataFrame, target_column: str, covariates: list[str]):\n",
    "    x_train = samples[covariates]\n",
    "    y_train = samples[target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator()\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 't_start': t_start, 't_end': time.time()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f167db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "covariates = [f'B{n + 1:02}' for n in range(64)]\n",
    "\n",
    "for target_column in class_name:\n",
    "    if (OUTPUT_DIR / f'rf.{target_column}.lz4').exists():\n",
    "        continue\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "        samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(n_fold=n_fold))\n",
    "\n",
    "        create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "        samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "        model = random_forest(samples, target_column, covariates)\n",
    "\n",
    "        model['#_fold'] = n_fold\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    joblib.dump(models, OUTPUT_DIR / f'rf.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957508e4",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661745d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator():\n",
    "    return xgb.XGBClassifier(n_jobs=-1, objective='binary:logistic', booster='gbtree', eval_metric='mlogloss', random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def random_forest(samples: pd.DataFrame, target_column: str, covariates: list[str]):\n",
    "    x_train = samples[covariates]\n",
    "    y_train = samples[target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator()\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 't_start': t_start, 't_end': time.time()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "covariates = [f'B{n + 1:02}' for n in range(64)]\n",
    "\n",
    "for target_column in class_name:\n",
    "    filename = f'xgb.{target_column}.lz4'\n",
    "\n",
    "    if (OUTPUT_DIR / filename).exists():\n",
    "        continue\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "        samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(n_fold=n_fold))\n",
    "\n",
    "        create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "        samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "        model = random_forest(samples, target_column, covariates)\n",
    "\n",
    "        model['#_fold'] = n_fold\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    joblib.dump(models, OUTPUT_DIR / filename, compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b38efc",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "736813f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator():\n",
    "    return lgb.LGBMClassifier(n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def random_forest(samples: pd.DataFrame, target_column: str, covariates: list[str]):\n",
    "    x_train = samples[covariates]\n",
    "    y_train = samples[target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator()\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 't_start': t_start, 't_end': time.time()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ab2831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 382162, number of negative: 2776846\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10370\n",
      "[LightGBM] [Info] Number of data points in the train set: 3159008, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120975 -> initscore=-1.983226\n",
      "[LightGBM] [Info] Start training from score -1.983226\n",
      "[LightGBM] [Info] Number of positive: 396559, number of negative: 2770676\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10398\n",
      "[LightGBM] [Info] Number of data points in the train set: 3167235, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.125207 -> initscore=-1.944022\n",
      "[LightGBM] [Info] Start training from score -1.944022\n",
      "[LightGBM] [Info] Number of positive: 378168, number of negative: 2747862\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.135860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10398\n",
      "[LightGBM] [Info] Number of data points in the train set: 3126030, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120974 -> initscore=-1.983240\n",
      "[LightGBM] [Info] Start training from score -1.983240\n",
      "[LightGBM] [Info] Number of positive: 394686, number of negative: 2801508\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.136406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10398\n",
      "[LightGBM] [Info] Number of data points in the train set: 3196194, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123486 -> initscore=-1.959823\n",
      "[LightGBM] [Info] Start training from score -1.959823\n",
      "[LightGBM] [Info] Number of positive: 376673, number of negative: 2759428\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.137226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10397\n",
      "[LightGBM] [Info] Number of data points in the train set: 3136101, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120109 -> initscore=-1.991401\n",
      "[LightGBM] [Info] Start training from score -1.991401\n",
      "[LightGBM] [Info] Number of positive: 917713, number of negative: 2776846\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.159618 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10407\n",
      "[LightGBM] [Info] Number of data points in the train set: 3694559, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248396 -> initscore=-1.107186\n",
      "[LightGBM] [Info] Start training from score -1.107186\n",
      "[LightGBM] [Info] Number of positive: 915286, number of negative: 2770676\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161707 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10415\n",
      "[LightGBM] [Info] Number of data points in the train set: 3685962, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248317 -> initscore=-1.107610\n",
      "[LightGBM] [Info] Start training from score -1.107610\n",
      "[LightGBM] [Info] Number of positive: 917540, number of negative: 2747862\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10415\n",
      "[LightGBM] [Info] Number of data points in the train set: 3665402, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.250325 -> initscore=-1.096882\n",
      "[LightGBM] [Info] Start training from score -1.096882\n",
      "[LightGBM] [Info] Number of positive: 876491, number of negative: 2801508\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10407\n",
      "[LightGBM] [Info] Number of data points in the train set: 3677999, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.238306 -> initscore=-1.161987\n",
      "[LightGBM] [Info] Start training from score -1.161987\n",
      "[LightGBM] [Info] Number of positive: 896294, number of negative: 2759428\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10406\n",
      "[LightGBM] [Info] Number of data points in the train set: 3655722, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.245176 -> initscore=-1.124510\n",
      "[LightGBM] [Info] Start training from score -1.124510\n",
      "[LightGBM] [Info] Number of positive: 765399, number of negative: 5554071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.281327 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10375\n",
      "[LightGBM] [Info] Number of data points in the train set: 6319470, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121118 -> initscore=-1.981889\n",
      "[LightGBM] [Info] Start training from score -1.981889\n",
      "[LightGBM] [Info] Number of positive: 791278, number of negative: 5542941\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.281833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10395\n",
      "[LightGBM] [Info] Number of data points in the train set: 6334219, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124921 -> initscore=-1.946631\n",
      "[LightGBM] [Info] Start training from score -1.946631\n",
      "[LightGBM] [Info] Number of positive: 759135, number of negative: 5494148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.270863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10388\n",
      "[LightGBM] [Info] Number of data points in the train set: 6253283, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121398 -> initscore=-1.979259\n",
      "[LightGBM] [Info] Start training from score -1.979259\n",
      "[LightGBM] [Info] Number of positive: 789686, number of negative: 5601291\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.280438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10402\n",
      "[LightGBM] [Info] Number of data points in the train set: 6390977, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123563 -> initscore=-1.959117\n",
      "[LightGBM] [Info] Start training from score -1.959117\n",
      "[LightGBM] [Info] Number of positive: 754985, number of negative: 5516301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.291034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10397\n",
      "[LightGBM] [Info] Number of data points in the train set: 6271286, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120388 -> initscore=-1.988765\n",
      "[LightGBM] [Info] Start training from score -1.988765\n",
      "[LightGBM] [Info] Number of positive: 1833067, number of negative: 5554071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.346353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10396\n",
      "[LightGBM] [Info] Number of data points in the train set: 7387138, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248143 -> initscore=-1.108541\n",
      "[LightGBM] [Info] Start training from score -1.108541\n",
      "[LightGBM] [Info] Number of positive: 1830985, number of negative: 5542941\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.325487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10427\n",
      "[LightGBM] [Info] Number of data points in the train set: 7373926, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248305 -> initscore=-1.107671\n",
      "[LightGBM] [Info] Start training from score -1.107671\n",
      "[LightGBM] [Info] Number of positive: 1835541, number of negative: 5494148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.328823 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10397\n",
      "[LightGBM] [Info] Number of data points in the train set: 7329689, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.250425 -> initscore=-1.096344\n",
      "[LightGBM] [Info] Start training from score -1.096344\n",
      "[LightGBM] [Info] Number of positive: 1754233, number of negative: 5601291\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.335152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10416\n",
      "[LightGBM] [Info] Number of data points in the train set: 7355524, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.238492 -> initscore=-1.160965\n",
      "[LightGBM] [Info] Start training from score -1.160965\n",
      "[LightGBM] [Info] Number of positive: 1793896, number of negative: 5516301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.330495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10420\n",
      "[LightGBM] [Info] Number of data points in the train set: 7310197, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.245396 -> initscore=-1.123318\n",
      "[LightGBM] [Info] Start training from score -1.123318\n",
      "[LightGBM] [Info] Number of positive: 1149273, number of negative: 8331985\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.414542 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10376\n",
      "[LightGBM] [Info] Number of data points in the train set: 9481258, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121215 -> initscore=-1.980972\n",
      "[LightGBM] [Info] Start training from score -1.980972\n",
      "[LightGBM] [Info] Number of positive: 1187110, number of negative: 8315463\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.422647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10388\n",
      "[LightGBM] [Info] Number of data points in the train set: 9502573, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124925 -> initscore=-1.946595\n",
      "[LightGBM] [Info] Start training from score -1.946595\n",
      "[LightGBM] [Info] Number of positive: 1137224, number of negative: 8243293\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.413570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10400\n",
      "[LightGBM] [Info] Number of data points in the train set: 9380517, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121233 -> initscore=-1.980810\n",
      "[LightGBM] [Info] Start training from score -1.980810\n",
      "[LightGBM] [Info] Number of positive: 1184528, number of negative: 8402106\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.427555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10401\n",
      "[LightGBM] [Info] Number of data points in the train set: 9586634, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123560 -> initscore=-1.959138\n",
      "[LightGBM] [Info] Start training from score -1.959138\n",
      "[LightGBM] [Info] Number of positive: 1129591, number of negative: 8278288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.428607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10399\n",
      "[LightGBM] [Info] Number of data points in the train set: 9407879, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120069 -> initscore=-1.991781\n",
      "[LightGBM] [Info] Start training from score -1.991781\n",
      "[LightGBM] [Info] Number of positive: 2747043, number of negative: 8331985\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.480462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10402\n",
      "[LightGBM] [Info] Number of data points in the train set: 11079028, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.247950 -> initscore=-1.109577\n",
      "[LightGBM] [Info] Start training from score -1.109577\n",
      "[LightGBM] [Info] Number of positive: 2745880, number of negative: 8315463\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.495399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10398\n",
      "[LightGBM] [Info] Number of data points in the train set: 11061343, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248241 -> initscore=-1.108015\n",
      "[LightGBM] [Info] Start training from score -1.108015\n",
      "[LightGBM] [Info] Number of positive: 2751580, number of negative: 8243293\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.477162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10397\n",
      "[LightGBM] [Info] Number of data points in the train set: 10994873, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.250260 -> initscore=-1.097225\n",
      "[LightGBM] [Info] Start training from score -1.097225\n",
      "[LightGBM] [Info] Number of positive: 2631662, number of negative: 8402106\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.479712 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11033768, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.238510 -> initscore=-1.160867\n",
      "[LightGBM] [Info] Start training from score -1.160867\n",
      "[LightGBM] [Info] Number of positive: 2689120, number of negative: 8278288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.489516 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10412\n",
      "[LightGBM] [Info] Number of data points in the train set: 10967408, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.245192 -> initscore=-1.124422\n",
      "[LightGBM] [Info] Start training from score -1.124422\n",
      "[LightGBM] [Info] Number of positive: 1532714, number of negative: 11108720\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.560558 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10369\n",
      "[LightGBM] [Info] Number of data points in the train set: 12641434, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121245 -> initscore=-1.980690\n",
      "[LightGBM] [Info] Start training from score -1.980690\n",
      "[LightGBM] [Info] Number of positive: 1584039, number of negative: 11086361\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.572456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10392\n",
      "[LightGBM] [Info] Number of data points in the train set: 12670400, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.125019 -> initscore=-1.945738\n",
      "[LightGBM] [Info] Start training from score -1.945738\n",
      "[LightGBM] [Info] Number of positive: 1516966, number of negative: 10987418\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.554520 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10389\n",
      "[LightGBM] [Info] Number of data points in the train set: 12504384, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121315 -> initscore=-1.980039\n",
      "[LightGBM] [Info] Start training from score -1.980039\n",
      "[LightGBM] [Info] Number of positive: 1579694, number of negative: 11200258\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.540216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10393\n",
      "[LightGBM] [Info] Number of data points in the train set: 12779952, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123607 -> initscore=-1.958706\n",
      "[LightGBM] [Info] Start training from score -1.958706\n",
      "[LightGBM] [Info] Number of positive: 1507935, number of negative: 11035086\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.557608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10407\n",
      "[LightGBM] [Info] Number of data points in the train set: 12543021, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120221 -> initscore=-1.990339\n",
      "[LightGBM] [Info] Start training from score -1.990339\n",
      "[LightGBM] [Info] Number of positive: 3664159, number of negative: 11108720\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.658440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10383\n",
      "[LightGBM] [Info] Number of data points in the train set: 14772879, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248033 -> initscore=-1.109132\n",
      "[LightGBM] [Info] Start training from score -1.109132\n",
      "[LightGBM] [Info] Number of positive: 3659983, number of negative: 11086361\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10418\n",
      "[LightGBM] [Info] Number of data points in the train set: 14746344, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248196 -> initscore=-1.108257\n",
      "[LightGBM] [Info] Start training from score -1.108257\n",
      "[LightGBM] [Info] Number of positive: 3671957, number of negative: 10987418\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.641193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10407\n",
      "[LightGBM] [Info] Number of data points in the train set: 14659375, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.250485 -> initscore=-1.096026\n",
      "[LightGBM] [Info] Start training from score -1.096026\n",
      "[LightGBM] [Info] Number of positive: 3511663, number of negative: 11200258\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.663345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10429\n",
      "[LightGBM] [Info] Number of data points in the train set: 14711921, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.238695 -> initscore=-1.159847\n",
      "[LightGBM] [Info] Start training from score -1.159847\n",
      "[LightGBM] [Info] Number of positive: 3586937, number of negative: 11035086\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.648047 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10411\n",
      "[LightGBM] [Info] Number of data points in the train set: 14622023, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.245311 -> initscore=-1.123781\n",
      "[LightGBM] [Info] Start training from score -1.123781\n"
     ]
    }
   ],
   "source": [
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "covariates = [f'B{n + 1:02}' for n in range(64)]\n",
    "\n",
    "for frac in [10, 20, 30, 40]:\n",
    "    for target_column in class_name:\n",
    "        filename = f'lgbm.{target_column}.frac_{frac}.lz4'\n",
    "\n",
    "        if (OUTPUT_DIR / filename).exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / f'samples.split_{n_fold}.frac_{frac}.train.pq')\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = random_forest(samples, target_column, covariates)\n",
    "\n",
    "            model['#_fold'] = n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / filename, compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969e638",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c951cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(n_neighbors, metric):\n",
    "    return KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "\n",
    "def knn_classifier(samples: pd.DataFrame, target_column: str, covariates: list[str], n_neighbors=3, metric='minkowski', sample_ratio=0.008):\n",
    "    x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
    "            lambda group: group.sample(frac=sample_ratio, random_state=RANDOM_STATE)\n",
    "        )[covariates]\n",
    "    y_train = samples.loc[x_train.index][target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator(n_neighbors, metric)\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 't_start': t_start, 't_end': time.time()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c81cf1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "for metric in ['minkowski', 'euclidean', 'manhattan', 'cosine']:\n",
    "    for target_column in class_name:\n",
    "        if (OUTPUT_DIR / f'knn.m_{metric}.{target_column}.lz4').exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(n_fold=n_fold))\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = knn_classifier(samples, target_column, covariates, metric=metric)\n",
    "\n",
    "            model['#_fold'] = n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / f'knn.m_{metric}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neighbors in [1, 3]:\n",
    "    for target_column in class_name:\n",
    "        if (OUTPUT_DIR / f'knn.nn_{n_neighbors}.{target_column}.lz4').exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(n_fold=n_fold))\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = knn_classifier(samples, target_column, covariates, n_neighbors=n_neighbors)\n",
    "\n",
    "            model['#_fold'] = n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / f'knn.nn_{n_neighbors}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Identify best model hyperparameters and retrain only that configuration.\n",
    "\n",
    "# Best model: model trained with n_neighbors=3 and metric='minkowski', observed to be the best performing configuration when isolated.\n",
    "\n",
    "for target_column in class_name:\n",
    "    if (OUTPUT_DIR / f'knn.nn_{n_neighbors}.{target_column}.lz4').exists():\n",
    "        continue\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "        samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(n_fold=n_fold))\n",
    "\n",
    "        create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "        samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "        model = knn_classifier(samples, target_column, covariates, n_neighbors=3, metrics='')\n",
    "\n",
    "        model['#_fold'] = n_fold\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    joblib.dump(models, OUTPUT_DIR / f'knn.nn_{n_neighbors}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b576247",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d593c3f",
   "metadata": {},
   "source": [
    "Para treinamento em quantidades maiores de amostras sera importante migrar para abordagens paralelas como propostas pelo framework [cuML SVM](https://medium.com/rapids-ai/fast-support-vector-classification-with-rapids-cuml-6e49f4a7d89e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d8da647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(kernel=\"linear\"):\n",
    "    return SVC(kernel=kernel, probability=True, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def linear_svc(samples: pd.DataFrame, target_column: str, covariates: list[str], kernel=\"linear\", sample_ratio=0.01):\n",
    "    x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
    "            lambda group: group.sample(frac=sample_ratio, random_state=RANDOM_STATE)\n",
    "        )[covariates]\n",
    "    y_train = samples.loc[x_train.index][target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator(kernel)\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 't_start': t_start, 't_end': time.time()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb76365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_23072\\2778031752.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_23072\\2778031752.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_23072\\2778031752.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_23072\\2778031752.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_23072\\2778031752.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_23072\\2778031752.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# metrics = ['linear', 'poly', 'rbf']\n",
    "\n",
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "for kernel in ['linear']:\n",
    "    for target_column in class_name:\n",
    "        if (OUTPUT_DIR / f'svc.k_{kernel}.{target_column}.lz4').exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(n_fold=n_fold))\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = linear_svc(samples, target_column, covariates, kernel)\n",
    "\n",
    "            model['#_fold'] =  n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / f'svc.k_{kernel}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83495a",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdbc4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator():\n",
    "    return LogisticRegression(n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def random_forest(samples: pd.DataFrame, target_column: str, covariates: list[str]):\n",
    "    x_train = samples[covariates]\n",
    "    y_train = samples[target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator()\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 't_start': t_start, 't_end': time.time()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda86e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "for kernel in ['linear', 'poly', 'rbf']:\n",
    "    for target_column in class_name:\n",
    "        if (OUTPUT_DIR / f'svc.k_{kernel}.{target_column}.lz4').exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN)\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = linear_svc(samples, target_column, covariates, kernel)\n",
    "\n",
    "            model['#_fold'] =  n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / f'svc.k_{kernel}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc6250",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Modelo sequencial com uma nica camada densa\n",
    "model = models.Sequential([\n",
    "    layers.Dense(512, input_shape=(64,), activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compila o modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Exibe o resumo do modelo\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
