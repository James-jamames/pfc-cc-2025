{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1df04bc-cd6c-4abf-9e6f-e7c1da2be89f",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeea2cf-18aa-476e-a4e6-ad0133952d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "INPUT_DIR = Path(\"../data/sample_splits\")\n",
    "OUTPUT_DIR = Path(\"../models\")\n",
    "\n",
    "SAMPLES_FN = \"samples.split_{split}.frac_{frac}.train.pq\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe796e8-48e6-4860-9dd9-263afa2baf81",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96819d5-9c27-4258-955f-9d05afde52a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a414c655-19d9-4c63-9fc3-0107d4c336c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "TARGET_COLUMN = 'class'\n",
    "SPATIAL_CROSS_VALIDATION_COLUMN = 'tile_id'\n",
    "\n",
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "covariates = [f'B{n:02}' for n in range(1, 65)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f99ac300-f5a3-46a4-bec0-65b718c93e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def target_ovo(samples: pd.DataFrame, class_name: str, class_a: list[int], class_b: list[int]):\n",
    "    remap_dict = {}\n",
    "    \n",
    "    remap_dict.update({val: 0.0 for val in class_a})\n",
    "    remap_dict.update({val: 1.0 for val in class_b})\n",
    "    \n",
    "    samples[class_name] = samples[TARGET_COLUMN].map(remap_dict)\n",
    "\n",
    "\n",
    "def create_ovo_class(samples: pd.DataFrame, class_name: list[str], class_values: list[tuple[list[int], list[int]]]):\n",
    "    class_data = dict(zip(class_name, class_values))\n",
    "    \n",
    "    for class_key in class_data:\n",
    "        value_a = class_data[class_key][0]\n",
    "        value_b = class_data[class_key][1]\n",
    "        \n",
    "        target_ovo(samples, class_key, value_a, value_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcfb3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_np(data_list: list[str], ratio_a: float = 0.8) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Separa uma lista de strings em dois arrays NumPy (80% para A, 20% para B).\n",
    "    \"\"\"\n",
    "    data_array = np.array(data_list)\n",
    "    \n",
    "    # 2. Calcular o tamanho de A (80%)\n",
    "    total_size = len(data_array)\n",
    "    # np.round() garante que o índice seja um número inteiro\n",
    "    size_a = int(np.round(total_size * ratio_a)) \n",
    "    \n",
    "    # 3. Criar uma permutação aleatória dos índices\n",
    "    # Isso garante que a divisão seja aleatória (evitando vieses)\n",
    "    indices = np.arange(total_size)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # 4. Dividir os índices\n",
    "    indices_a = indices[:size_a]\n",
    "    indices_b = indices[size_a:]\n",
    "    \n",
    "    # 5. Aplicar os índices para obter os arrays\n",
    "    array_a = data_array[indices_a]\n",
    "    array_b = data_array[indices_b]\n",
    "    \n",
    "    return array_a, array_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260a915",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c61a1ee-96a1-4942-b9d7-daf72ee4c1a5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_estimator():\n",
    "    return RandomForestClassifier(n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def random_forest(samples: pd.DataFrame, target_column: str, covariates: list[str]):\n",
    "    x_train = samples[covariates]\n",
    "    y_train = samples[target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator()\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 'training_time': time.time() - t_start}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f167db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "covariates = [f'B{n + 1:02}' for n in range(64)]\n",
    "\n",
    "for frac in [10, 20, 30, 40]:\n",
    "    for target_column in class_name:\n",
    "        filename = f'rf.frac_{frac}.{target_column}.lz4'\n",
    "\n",
    "        if (OUTPUT_DIR / filename).exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(split=n_fold, frac=frac))\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = random_forest(samples, target_column, covariates)\n",
    "\n",
    "            model['#_fold'] = n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / filename, compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957508e4",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661745d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator():\n",
    "    return xgb.XGBClassifier(n_jobs=-1, objective='binary:logistic', booster='gbtree', eval_metric='mlogloss', random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def xgb(samples: pd.DataFrame, target_column: str, covariates: list[str]):\n",
    "    x_train = samples[covariates]\n",
    "    y_train = samples[target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator()\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 'training_time': time.time() - t_start}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "covariates = [f'B{n + 1:02}' for n in range(64)]\n",
    "\n",
    "for target_column in class_name:\n",
    "    filename = f'xgb.{target_column}.lz4'\n",
    "\n",
    "    if (OUTPUT_DIR / filename).exists():\n",
    "        continue\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "        samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(split=n_fold, frac=20))\n",
    "\n",
    "        create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "        samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "        model = xgb(samples, target_column, covariates)\n",
    "\n",
    "        model['#_fold'] = n_fold\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    joblib.dump(models, OUTPUT_DIR / filename, compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b38efc",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736813f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator():\n",
    "    return lgb.LGBMClassifier(n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def lgbm(samples: pd.DataFrame, target_column: str, covariates: list[str]):\n",
    "    x_train = samples[covariates]\n",
    "    y_train = samples[target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator()\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 'training_time': time.time() - t_start}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab2831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 622906, number of negative: 4399646\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10363\n",
      "[LightGBM] [Info] Number of data points in the train set: 5022552, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124022 -> initscore=-1.954884\n",
      "[LightGBM] [Info] Start training from score -1.954884\n",
      "[LightGBM] [Info] Number of positive: 624842, number of negative: 4438208\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.230727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10374\n",
      "[LightGBM] [Info] Number of data points in the train set: 5063050, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123412 -> initscore=-1.960507\n",
      "[LightGBM] [Info] Start training from score -1.960507\n",
      "[LightGBM] [Info] Number of positive: 577521, number of negative: 4423684\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10387\n",
      "[LightGBM] [Info] Number of data points in the train set: 5001205, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115476 -> initscore=-2.035983\n",
      "[LightGBM] [Info] Start training from score -2.035983\n",
      "[LightGBM] [Info] Number of positive: 657555, number of negative: 4476358\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10403\n",
      "[LightGBM] [Info] Number of data points in the train set: 5133913, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.128081 -> initscore=-1.918037\n",
      "[LightGBM] [Info] Start training from score -1.918037\n",
      "[LightGBM] [Info] Number of positive: 619913, number of negative: 4392074\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.213466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10384\n",
      "[LightGBM] [Info] Number of data points in the train set: 5011987, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123686 -> initscore=-1.957978\n",
      "[LightGBM] [Info] Start training from score -1.957978\n",
      "[LightGBM] [Info] Number of positive: 1440700, number of negative: 4410453\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.253573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10376\n",
      "[LightGBM] [Info] Number of data points in the train set: 5851153, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.246225 -> initscore=-1.118848\n",
      "[LightGBM] [Info] Start training from score -1.118848\n",
      "[LightGBM] [Info] Number of positive: 1440047, number of negative: 4443175\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10398\n",
      "[LightGBM] [Info] Number of data points in the train set: 5883222, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244772 -> initscore=-1.126693\n",
      "[LightGBM] [Info] Start training from score -1.126693\n",
      "[LightGBM] [Info] Number of positive: 1448178, number of negative: 4473873\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.257606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10394\n",
      "[LightGBM] [Info] Number of data points in the train set: 5922051, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244540 -> initscore=-1.127948\n",
      "[LightGBM] [Info] Start training from score -1.127948\n",
      "[LightGBM] [Info] Number of positive: 1399281, number of negative: 4480885\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10396\n",
      "[LightGBM] [Info] Number of data points in the train set: 5880166, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.237966 -> initscore=-1.163862\n",
      "[LightGBM] [Info] Start training from score -1.163862\n",
      "[LightGBM] [Info] Number of positive: 1448194, number of negative: 4368298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.252006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10395\n",
      "[LightGBM] [Info] Number of data points in the train set: 5816492, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248981 -> initscore=-1.104056\n",
      "[LightGBM] [Info] Start training from score -1.104056\n"
     ]
    }
   ],
   "source": [
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "covariates = [f'B{n + 1:02}' for n in range(64)]\n",
    "\n",
    "for target_column in class_name:\n",
    "    filename = f'lgbm.{target_column}.lz4'\n",
    "\n",
    "    if (OUTPUT_DIR / filename).exists():\n",
    "        continue\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "        samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(split=n_fold, frac=20))\n",
    "\n",
    "        create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "        samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "        model = lgbm(samples, target_column, covariates)\n",
    "\n",
    "        model['#_fold'] = n_fold\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    joblib.dump(models, OUTPUT_DIR / filename, compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969e638",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c951cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(n_neighbors, metric):\n",
    "    return KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "\n",
    "def knn_classifier(samples: pd.DataFrame, target_column: str, covariates: list[str], n_neighbors=3, metric='minkowski', sample_ratio=0.008):\n",
    "    x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
    "            lambda group: group.sample(frac=sample_ratio, random_state=RANDOM_STATE)\n",
    "        )[covariates]\n",
    "    y_train = samples.loc[x_train.index][target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator(n_neighbors, metric)\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 'training_time': time.time() - t_start}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c81cf1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13444\\685623306.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "for metric in ['minkowski', 'euclidean', 'manhattan', 'cosine']:\n",
    "    for target_column in class_name:\n",
    "        if (OUTPUT_DIR / f'knn.m_{metric}.{target_column}.lz4').exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(n_fold=n_fold))\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = knn_classifier(samples, target_column, covariates, metric=metric)\n",
    "\n",
    "            model['#_fold'] = n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / f'knn.m_{metric}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neighbors in [1, 3]:\n",
    "    for target_column in class_name:\n",
    "        if (OUTPUT_DIR / f'knn.nn_{n_neighbors}.{target_column}.lz4').exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(n_fold=n_fold))\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = knn_classifier(samples, target_column, covariates, n_neighbors=n_neighbors)\n",
    "\n",
    "            model['#_fold'] = n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / f'knn.nn_{n_neighbors}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Identify best model hyperparameters and retrain only that configuration.\n",
    "\n",
    "# Best model: model trained with n_neighbors=3 and metric='minkowski', observed to be the best performing configuration when isolated.\n",
    "\n",
    "for target_column in class_name:\n",
    "    if (OUTPUT_DIR / f'knn.nn_{n_neighbors}.{target_column}.lz4').exists():\n",
    "        continue\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "        samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(n_fold=n_fold))\n",
    "\n",
    "        create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "        samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "        model = knn_classifier(samples, target_column, covariates, n_neighbors=3, metrics='')\n",
    "\n",
    "        model['#_fold'] = n_fold\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    joblib.dump(models, OUTPUT_DIR / f'knn.nn_{n_neighbors}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b576247",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d593c3f",
   "metadata": {},
   "source": [
    "Para treinamento em quantidades maiores de amostras sera importante migrar para abordagens paralelas como propostas pelo framework [cuML SVM](https://medium.com/rapids-ai/fast-support-vector-classification-with-rapids-cuml-6e49f4a7d89e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8da647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(kernel=\"linear\"):\n",
    "    return SVC(kernel=kernel, probability=True, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def linear_svc(samples: pd.DataFrame, target_column: str, covariates: list[str], kernel=\"linear\", sample_ratio=0.01):\n",
    "    x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
    "            lambda group: group.sample(frac=sample_ratio, random_state=RANDOM_STATE)\n",
    "        )[covariates]\n",
    "    y_train = samples.loc[x_train.index][target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator(kernel)\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 'training_time': time.time() - t_start}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb76365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_23072\\2778031752.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_23072\\2778031752.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_23072\\2778031752.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_23072\\2778031752.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_23072\\2778031752.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_23072\\2778031752.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# metrics = ['linear', 'poly', 'rbf']\n",
    "\n",
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "for kernel in ['linear']:\n",
    "    for target_column in class_name:\n",
    "        if (OUTPUT_DIR / f'svc.k_{kernel}.{target_column}.lz4').exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(n_fold=n_fold))\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = linear_svc(samples, target_column, covariates, kernel)\n",
    "\n",
    "            model['#_fold'] =  n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / f'svc.k_{kernel}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83495a",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdbc4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator():\n",
    "    return LogisticRegression(n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def random_forest(samples: pd.DataFrame, target_column: str, covariates: list[str]):\n",
    "    x_train = samples[covariates]\n",
    "    y_train = samples[target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator()\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 'training_time': time.time() - t_start}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda86e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "for kernel in ['linear', 'poly', 'rbf']:\n",
    "    for target_column in class_name:\n",
    "        if (OUTPUT_DIR / f'svc.k_{kernel}.{target_column}.lz4').exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN)\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = linear_svc(samples, target_column, covariates, kernel)\n",
    "\n",
    "            model['#_fold'] =  n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / f'svc.k_{kernel}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc6250",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93c666ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 15:33:26.810542: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-12 15:33:26.818240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762972406.827880    2272 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762972406.831672    2272 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1762972406.839884    2272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762972406.839898    2272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762972406.839899    2272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762972406.839900    2272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-12 15:33:26.842675: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão do TensorFlow: 2.19.1\n",
      "Número de GPUs encontradas: 1\n",
      "Detalhes: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "✅ GPU está pronta para uso (memory growth ativado).\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Versão do TensorFlow: {tf.__version__}\")\n",
    "\n",
    "# Lista os dispositivos físicos que o TensorFlow pode usar\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(f\"Número de GPUs encontradas: {len(gpus)}\")\n",
    "    print(f\"Detalhes: {gpus}\")\n",
    "    \n",
    "    try:\n",
    "        # Tenta alocar memória na primeira GPU para confirmar que está funcional\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"✅ GPU está pronta para uso (memory growth ativado).\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"❌ Erro ao inicializar a GPU: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ NENHUMA GPU compatível foi encontrada pelo TensorFlow.\")\n",
    "    print(\"O modelo irá treinar usando a CPU (muito mais lento).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b32b7280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiago/miniforge3/envs/tensorflow/lib/python3.12/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1762972409.029788    2272 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13065 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5080, pci bus id: 0000:02:00.0, compute capability: 12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Modelo sequencial com uma única camada densa\n",
    "# melhor modelo com 7 layers 256, activation='relu' e dropout=0.4; early_stopping -> default com 3 espera. Sempre apenas um epoch.\n",
    "model = models.Sequential([\n",
    "    layers.Dense(512, input_shape=(64,), activation='sigmoid'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(512, activation='sigmoid'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(512, activation='sigmoid'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(512, activation='sigmoid'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compila o modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='crossentropy',\n",
    "    metrics=['precision', 'recall']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "591524e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_train = pd.read_parquet(INPUT_DIR / 'samples.split_01.frac_20.train.pq')\n",
    "\n",
    "create_ovo_class(samples_train, class_name, class_values)\n",
    "\n",
    "samples_train = samples_train[np.logical_not(np.isnan(samples_train['oxc']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb65ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_valid = pd.read_parquet(INPUT_DIR / 'samples.split_01.test.pq')\n",
    "\n",
    "create_ovo_class(samples_valid, class_name, class_values)\n",
    "\n",
    "samples_valid = samples_valid[np.logical_not(np.isnan(samples_valid['oxc']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b716fb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762972427.154476    2377 service.cc:152] XLA service 0x767fb40099c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1762972427.154504    2377 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 5080, Compute Capability 12.0\n",
      "2025-11-12 15:33:47.184483: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1762972427.363750    2377 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 14/772\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.8955 - precision: 0.1676 - recall: 0.6456  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762972428.678224    2377 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - loss: 0.2373 - precision: 0.6064 - recall: 0.5789 - val_loss: 0.2258 - val_precision: 0.6356 - val_recall: 0.6492\n",
      "Epoch 2/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1669 - precision: 0.7753 - recall: 0.6487 - val_loss: 0.2242 - val_precision: 0.6823 - val_recall: 0.5868\n",
      "Epoch 3/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.1515 - precision: 0.7968 - recall: 0.6873 - val_loss: 0.2348 - val_precision: 0.6848 - val_recall: 0.5568\n",
      "Epoch 4/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1411 - precision: 0.8121 - recall: 0.7129 - val_loss: 0.2400 - val_precision: 0.6939 - val_recall: 0.5557\n",
      "Epoch 5/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1330 - precision: 0.8228 - recall: 0.7319 - val_loss: 0.2463 - val_precision: 0.6911 - val_recall: 0.5551\n",
      "Epoch 6/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.1275 - precision: 0.8297 - recall: 0.7441 - val_loss: 0.2536 - val_precision: 0.6696 - val_recall: 0.5722\n",
      "Epoch 7/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1235 - precision: 0.8354 - recall: 0.7537 - val_loss: 0.2608 - val_precision: 0.6848 - val_recall: 0.5474\n",
      "Epoch 8/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1202 - precision: 0.8394 - recall: 0.7610 - val_loss: 0.2626 - val_precision: 0.6661 - val_recall: 0.5626\n",
      "Epoch 9/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1178 - precision: 0.8424 - recall: 0.7663 - val_loss: 0.2674 - val_precision: 0.6787 - val_recall: 0.5614\n",
      "Epoch 10/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1162 - precision: 0.8442 - recall: 0.7699 - val_loss: 0.2696 - val_precision: 0.6703 - val_recall: 0.5719\n",
      "Epoch 11/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1147 - precision: 0.8463 - recall: 0.7730 - val_loss: 0.2682 - val_precision: 0.6669 - val_recall: 0.5719\n",
      "Epoch 12/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1135 - precision: 0.8474 - recall: 0.7756 - val_loss: 0.2757 - val_precision: 0.6631 - val_recall: 0.5764\n",
      "Epoch 13/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.1127 - precision: 0.8484 - recall: 0.7777 - val_loss: 0.2754 - val_precision: 0.6622 - val_recall: 0.5751\n",
      "Epoch 14/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1119 - precision: 0.8498 - recall: 0.7793 - val_loss: 0.2741 - val_precision: 0.6632 - val_recall: 0.5735\n",
      "Epoch 15/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1110 - precision: 0.8504 - recall: 0.7813 - val_loss: 0.2792 - val_precision: 0.6721 - val_recall: 0.5569\n",
      "Epoch 16/50\n",
      "\u001b[1m772/772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1107 - precision: 0.8508 - recall: 0.7820 - val_loss: 0.2743 - val_precision: 0.6783 - val_recall: 0.5619\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7682860872f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_precision',     # Métrica a monitorizar\n",
    "    min_delta=0.05,\n",
    "    patience=10,            # Número de épocas sem melhoria antes de parar\n",
    "    verbose=1,              # Imprime uma mensagem quando para\n",
    "    mode='max',             # 'min' porque a perda (loss) deve minimizar\n",
    "    restore_best_weights=True # Restaura os melhores pesos encontrados\n",
    ")\n",
    "\n",
    "model.fit(samples_train[covariates], samples_train['oxc'], epochs=50,\n",
    "            batch_size=8192, callbacks=[early_stopping_monitor],\n",
    "            validation_data=(samples_valid[covariates], samples_valid['oxc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89995269-d616-4da7-beae-b0586467b497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
