{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1df04bc-cd6c-4abf-9e6f-e7c1da2be89f",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbeea2cf-18aa-476e-a4e6-ad0133952d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "INPUT_DIR = Path(\"../data/sample_splits\")\n",
    "OUTPUT_DIR = Path(\"../models\")\n",
    "\n",
    "SAMPLES_FN = \"samples.split_{split}.frac_{frac}.train.pq\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe796e8-48e6-4860-9dd9-263afa2baf81",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96819d5-9c27-4258-955f-9d05afde52a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a414c655-19d9-4c63-9fc3-0107d4c336c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "TARGET_COLUMN = 'class'\n",
    "SPATIAL_CROSS_VALIDATION_COLUMN = 'tile_id'\n",
    "\n",
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "covariates = [f'B{n:02}' for n in range(1, 65)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f99ac300-f5a3-46a4-bec0-65b718c93e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def target_ovo(samples: pd.DataFrame, class_name: str, class_a: list[int], class_b: list[int]):\n",
    "    remap_dict = {}\n",
    "    \n",
    "    remap_dict.update({val: 0.0 for val in class_a})\n",
    "    remap_dict.update({val: 1.0 for val in class_b})\n",
    "    \n",
    "    samples[class_name] = samples[TARGET_COLUMN].map(remap_dict)\n",
    "\n",
    "\n",
    "def create_ovo_class(samples: pd.DataFrame, class_name: list[str], class_values: list[tuple[list[int], list[int]]]):\n",
    "    class_data = dict(zip(class_name, class_values))\n",
    "    \n",
    "    for class_key in class_data:\n",
    "        value_a = class_data[class_key][0]\n",
    "        value_b = class_data[class_key][1]\n",
    "        \n",
    "        target_ovo(samples, class_key, value_a, value_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcfb3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_np(data_list: list[str], ratio_a: float = 0.8) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Separa uma lista de strings em dois arrays NumPy (80% para A, 20% para B).\n",
    "    \"\"\"\n",
    "    data_array = np.array(data_list)\n",
    "    \n",
    "    # 2. Calcular o tamanho de A (80%)\n",
    "    total_size = len(data_array)\n",
    "    # np.round() garante que o índice seja um número inteiro\n",
    "    size_a = int(np.round(total_size * ratio_a)) \n",
    "    \n",
    "    # 3. Criar uma permutação aleatória dos índices\n",
    "    # Isso garante que a divisão seja aleatória (evitando vieses)\n",
    "    indices = np.arange(total_size)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # 4. Dividir os índices\n",
    "    indices_a = indices[:size_a]\n",
    "    indices_b = indices[size_a:]\n",
    "    \n",
    "    # 5. Aplicar os índices para obter os arrays\n",
    "    array_a = data_array[indices_a]\n",
    "    array_b = data_array[indices_b]\n",
    "    \n",
    "    return array_a, array_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260a915",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c61a1ee-96a1-4942-b9d7-daf72ee4c1a5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_estimator():\n",
    "    return RandomForestClassifier(n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def random_forest(samples: pd.DataFrame, target_column: str, covariates: list[str]):\n",
    "    x_train = samples[covariates]\n",
    "    y_train = samples[target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator()\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 'training_time': time.time() - t_start}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f167db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "covariates = [f'B{n + 1:02}' for n in range(64)]\n",
    "\n",
    "for frac in [10, 20, 30, 40]:\n",
    "    for target_column in class_name:\n",
    "        filename = f'rf.frac_{frac}.{target_column}.lz4'\n",
    "\n",
    "        if (OUTPUT_DIR / filename).exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(split=n_fold, frac=frac))\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = random_forest(samples, target_column, covariates)\n",
    "\n",
    "            model['#_fold'] = n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / filename, compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957508e4",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661745d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator():\n",
    "    return xgb.XGBClassifier(n_jobs=-1, objective='binary:logistic', booster='gbtree', eval_metric='mlogloss', random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def xgb(samples: pd.DataFrame, target_column: str, covariates: list[str]):\n",
    "    x_train = samples[covariates]\n",
    "    y_train = samples[target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator()\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 'training_time': time.time() - t_start}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "covariates = [f'B{n + 1:02}' for n in range(64)]\n",
    "\n",
    "for target_column in class_name:\n",
    "    filename = f'xgb.{target_column}.lz4'\n",
    "\n",
    "    if (OUTPUT_DIR / filename).exists():\n",
    "        continue\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "        samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(split=n_fold, frac=20))\n",
    "\n",
    "        create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "        samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "        model = xgb(samples, target_column, covariates)\n",
    "\n",
    "        model['#_fold'] = n_fold\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    joblib.dump(models, OUTPUT_DIR / filename, compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b38efc",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "736813f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator():\n",
    "    return lgb.LGBMClassifier(n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def lgbm(samples: pd.DataFrame, target_column: str, covariates: list[str]):\n",
    "    x_train = samples[covariates]\n",
    "    y_train = samples[target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator()\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 'training_time': time.time() - t_start}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61ab2831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 382162, number of negative: 2776846\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.130096 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10370\n",
      "[LightGBM] [Info] Number of data points in the train set: 3159008, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120975 -> initscore=-1.983226\n",
      "[LightGBM] [Info] Start training from score -1.983226\n",
      "[LightGBM] [Info] Number of positive: 396559, number of negative: 2770676\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.135141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10398\n",
      "[LightGBM] [Info] Number of data points in the train set: 3167235, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.125207 -> initscore=-1.944022\n",
      "[LightGBM] [Info] Start training from score -1.944022\n",
      "[LightGBM] [Info] Number of positive: 378168, number of negative: 2747862\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10398\n",
      "[LightGBM] [Info] Number of data points in the train set: 3126030, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120974 -> initscore=-1.983240\n",
      "[LightGBM] [Info] Start training from score -1.983240\n",
      "[LightGBM] [Info] Number of positive: 394686, number of negative: 2801508\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.136827 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10398\n",
      "[LightGBM] [Info] Number of data points in the train set: 3196194, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123486 -> initscore=-1.959823\n",
      "[LightGBM] [Info] Start training from score -1.959823\n",
      "[LightGBM] [Info] Number of positive: 376673, number of negative: 2759428\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10397\n",
      "[LightGBM] [Info] Number of data points in the train set: 3136101, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120109 -> initscore=-1.991401\n",
      "[LightGBM] [Info] Start training from score -1.991401\n",
      "[LightGBM] [Info] Number of positive: 917713, number of negative: 2776846\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.158395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10407\n",
      "[LightGBM] [Info] Number of data points in the train set: 3694559, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248396 -> initscore=-1.107186\n",
      "[LightGBM] [Info] Start training from score -1.107186\n",
      "[LightGBM] [Info] Number of positive: 915286, number of negative: 2770676\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.154989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10415\n",
      "[LightGBM] [Info] Number of data points in the train set: 3685962, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248317 -> initscore=-1.107610\n",
      "[LightGBM] [Info] Start training from score -1.107610\n",
      "[LightGBM] [Info] Number of positive: 917540, number of negative: 2747862\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.154297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10415\n",
      "[LightGBM] [Info] Number of data points in the train set: 3665402, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.250325 -> initscore=-1.096882\n",
      "[LightGBM] [Info] Start training from score -1.096882\n",
      "[LightGBM] [Info] Number of positive: 876491, number of negative: 2801508\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10407\n",
      "[LightGBM] [Info] Number of data points in the train set: 3677999, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.238306 -> initscore=-1.161987\n",
      "[LightGBM] [Info] Start training from score -1.161987\n",
      "[LightGBM] [Info] Number of positive: 896294, number of negative: 2759428\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.154994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10406\n",
      "[LightGBM] [Info] Number of data points in the train set: 3655722, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.245176 -> initscore=-1.124510\n",
      "[LightGBM] [Info] Start training from score -1.124510\n",
      "[LightGBM] [Info] Number of positive: 765399, number of negative: 5554071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.280040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10375\n",
      "[LightGBM] [Info] Number of data points in the train set: 6319470, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121118 -> initscore=-1.981889\n",
      "[LightGBM] [Info] Start training from score -1.981889\n",
      "[LightGBM] [Info] Number of positive: 791278, number of negative: 5542941\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10395\n",
      "[LightGBM] [Info] Number of data points in the train set: 6334219, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124921 -> initscore=-1.946631\n",
      "[LightGBM] [Info] Start training from score -1.946631\n",
      "[LightGBM] [Info] Number of positive: 759135, number of negative: 5494148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.268743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10388\n",
      "[LightGBM] [Info] Number of data points in the train set: 6253283, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121398 -> initscore=-1.979259\n",
      "[LightGBM] [Info] Start training from score -1.979259\n",
      "[LightGBM] [Info] Number of positive: 789686, number of negative: 5601291\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10402\n",
      "[LightGBM] [Info] Number of data points in the train set: 6390977, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123563 -> initscore=-1.959117\n",
      "[LightGBM] [Info] Start training from score -1.959117\n",
      "[LightGBM] [Info] Number of positive: 754985, number of negative: 5516301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.272453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10397\n",
      "[LightGBM] [Info] Number of data points in the train set: 6271286, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120388 -> initscore=-1.988765\n",
      "[LightGBM] [Info] Start training from score -1.988765\n",
      "[LightGBM] [Info] Number of positive: 1833067, number of negative: 5554071\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10396\n",
      "[LightGBM] [Info] Number of data points in the train set: 7387138, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248143 -> initscore=-1.108541\n",
      "[LightGBM] [Info] Start training from score -1.108541\n",
      "[LightGBM] [Info] Number of positive: 1830985, number of negative: 5542941\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10427\n",
      "[LightGBM] [Info] Number of data points in the train set: 7373926, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248305 -> initscore=-1.107671\n",
      "[LightGBM] [Info] Start training from score -1.107671\n",
      "[LightGBM] [Info] Number of positive: 1835541, number of negative: 5494148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.316797 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10397\n",
      "[LightGBM] [Info] Number of data points in the train set: 7329689, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.250425 -> initscore=-1.096344\n",
      "[LightGBM] [Info] Start training from score -1.096344\n",
      "[LightGBM] [Info] Number of positive: 1754233, number of negative: 5601291\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.318434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10416\n",
      "[LightGBM] [Info] Number of data points in the train set: 7355524, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.238492 -> initscore=-1.160965\n",
      "[LightGBM] [Info] Start training from score -1.160965\n",
      "[LightGBM] [Info] Number of positive: 1793896, number of negative: 5516301\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057367 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10420\n",
      "[LightGBM] [Info] Number of data points in the train set: 7310197, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.245396 -> initscore=-1.123318\n",
      "[LightGBM] [Info] Start training from score -1.123318\n",
      "[LightGBM] [Info] Number of positive: 1149273, number of negative: 8331985\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073660 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10376\n",
      "[LightGBM] [Info] Number of data points in the train set: 9481258, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121215 -> initscore=-1.980972\n",
      "[LightGBM] [Info] Start training from score -1.980972\n",
      "[LightGBM] [Info] Number of positive: 1187110, number of negative: 8315463\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.410058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10388\n",
      "[LightGBM] [Info] Number of data points in the train set: 9502573, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124925 -> initscore=-1.946595\n",
      "[LightGBM] [Info] Start training from score -1.946595\n",
      "[LightGBM] [Info] Number of positive: 1137224, number of negative: 8243293\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.403910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10400\n",
      "[LightGBM] [Info] Number of data points in the train set: 9380517, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121233 -> initscore=-1.980810\n",
      "[LightGBM] [Info] Start training from score -1.980810\n",
      "[LightGBM] [Info] Number of positive: 1184528, number of negative: 8402106\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.418900 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10401\n",
      "[LightGBM] [Info] Number of data points in the train set: 9586634, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123560 -> initscore=-1.959138\n",
      "[LightGBM] [Info] Start training from score -1.959138\n",
      "[LightGBM] [Info] Number of positive: 1129591, number of negative: 8278288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.407478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10399\n",
      "[LightGBM] [Info] Number of data points in the train set: 9407879, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120069 -> initscore=-1.991781\n",
      "[LightGBM] [Info] Start training from score -1.991781\n",
      "[LightGBM] [Info] Number of positive: 2747043, number of negative: 8331985\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10402\n",
      "[LightGBM] [Info] Number of data points in the train set: 11079028, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.247950 -> initscore=-1.109577\n",
      "[LightGBM] [Info] Start training from score -1.109577\n",
      "[LightGBM] [Info] Number of positive: 2745880, number of negative: 8315463\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.470360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10398\n",
      "[LightGBM] [Info] Number of data points in the train set: 11061343, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248241 -> initscore=-1.108015\n",
      "[LightGBM] [Info] Start training from score -1.108015\n",
      "[LightGBM] [Info] Number of positive: 2751580, number of negative: 8243293\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.467176 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10397\n",
      "[LightGBM] [Info] Number of data points in the train set: 10994873, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.250260 -> initscore=-1.097225\n",
      "[LightGBM] [Info] Start training from score -1.097225\n",
      "[LightGBM] [Info] Number of positive: 2631662, number of negative: 8402106\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.078501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10422\n",
      "[LightGBM] [Info] Number of data points in the train set: 11033768, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.238510 -> initscore=-1.160867\n",
      "[LightGBM] [Info] Start training from score -1.160867\n",
      "[LightGBM] [Info] Number of positive: 2689120, number of negative: 8278288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.474491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10412\n",
      "[LightGBM] [Info] Number of data points in the train set: 10967408, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.245192 -> initscore=-1.124422\n",
      "[LightGBM] [Info] Start training from score -1.124422\n",
      "[LightGBM] [Info] Number of positive: 1532714, number of negative: 11108720\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10369\n",
      "[LightGBM] [Info] Number of data points in the train set: 12641434, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121245 -> initscore=-1.980690\n",
      "[LightGBM] [Info] Start training from score -1.980690\n",
      "[LightGBM] [Info] Number of positive: 1584039, number of negative: 11086361\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.541103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10392\n",
      "[LightGBM] [Info] Number of data points in the train set: 12670400, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.125019 -> initscore=-1.945738\n",
      "[LightGBM] [Info] Start training from score -1.945738\n",
      "[LightGBM] [Info] Number of positive: 1516966, number of negative: 10987418\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10389\n",
      "[LightGBM] [Info] Number of data points in the train set: 12504384, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121315 -> initscore=-1.980039\n",
      "[LightGBM] [Info] Start training from score -1.980039\n",
      "[LightGBM] [Info] Number of positive: 1579694, number of negative: 11200258\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.582187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10393\n",
      "[LightGBM] [Info] Number of data points in the train set: 12779952, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123607 -> initscore=-1.958706\n",
      "[LightGBM] [Info] Start training from score -1.958706\n",
      "[LightGBM] [Info] Number of positive: 1507935, number of negative: 11035086\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10407\n",
      "[LightGBM] [Info] Number of data points in the train set: 12543021, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120221 -> initscore=-1.990339\n",
      "[LightGBM] [Info] Start training from score -1.990339\n",
      "[LightGBM] [Info] Number of positive: 3664159, number of negative: 11108720\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10383\n",
      "[LightGBM] [Info] Number of data points in the train set: 14772879, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248033 -> initscore=-1.109132\n",
      "[LightGBM] [Info] Start training from score -1.109132\n",
      "[LightGBM] [Info] Number of positive: 3659983, number of negative: 11086361\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.631149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10418\n",
      "[LightGBM] [Info] Number of data points in the train set: 14746344, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.248196 -> initscore=-1.108257\n",
      "[LightGBM] [Info] Start training from score -1.108257\n",
      "[LightGBM] [Info] Number of positive: 3671957, number of negative: 10987418\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10407\n",
      "[LightGBM] [Info] Number of data points in the train set: 14659375, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.250485 -> initscore=-1.096026\n",
      "[LightGBM] [Info] Start training from score -1.096026\n",
      "[LightGBM] [Info] Number of positive: 3511663, number of negative: 11200258\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.623608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10429\n",
      "[LightGBM] [Info] Number of data points in the train set: 14711921, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.238695 -> initscore=-1.159847\n",
      "[LightGBM] [Info] Start training from score -1.159847\n",
      "[LightGBM] [Info] Number of positive: 3586937, number of negative: 11035086\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10411\n",
      "[LightGBM] [Info] Number of data points in the train set: 14622023, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.245311 -> initscore=-1.123781\n",
      "[LightGBM] [Info] Start training from score -1.123781\n"
     ]
    }
   ],
   "source": [
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "covariates = [f'B{n + 1:02}' for n in range(64)]\n",
    "\n",
    "for frac in [10, 20, 30, 40]:\n",
    "    for target_column in class_name:\n",
    "        filename = f'lgbm.{target_column}.frac_{frac}.lz4'\n",
    "\n",
    "        if (OUTPUT_DIR / filename).exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(split=n_fold, frac=frac))\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = lgbm(samples, target_column, covariates)\n",
    "\n",
    "            model['#_fold'] = n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / filename, compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969e638",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c951cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(n_neighbors, metric):\n",
    "    return KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "\n",
    "def knn_classifier(samples: pd.DataFrame, target_column: str, covariates: list[str], n_neighbors=3, metric='minkowski', sample_ratio=0.008):\n",
    "    x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
    "            lambda group: group.sample(frac=sample_ratio, random_state=RANDOM_STATE)\n",
    "        )[covariates]\n",
    "    y_train = samples.loc[x_train.index][target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator(n_neighbors, metric)\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 'training_time': time.time() - t_start}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81cf1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['minkowski', 'euclidean', 'manhattan', 'cosine']:\n",
    "    for target_column in class_name:\n",
    "        if (OUTPUT_DIR / f'knn.m_{metric}.{target_column}.lz4').exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(n_fold=n_fold))\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = knn_classifier(samples, target_column, covariates, metric=metric)\n",
    "\n",
    "            model['#_fold'] = n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / f'knn.m_{metric}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neighbors in [1, 3]:\n",
    "    for target_column in class_name:\n",
    "        if (OUTPUT_DIR / f'knn.nn_{n_neighbors}.{target_column}.lz4').exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(n_fold=n_fold))\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = knn_classifier(samples, target_column, covariates, n_neighbors=n_neighbors)\n",
    "\n",
    "            model['#_fold'] = n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / f'knn.nn_{n_neighbors}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b11a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Identify best model hyperparameters and retrain only that configuration.\n",
    "\n",
    "# Best model: model trained with n_neighbors=3 and metric='minkowski', observed to be the best performing configuration when isolated.\n",
    "\n",
    "for target_column in class_name:\n",
    "    if (OUTPUT_DIR / f'knn.nn_{n_neighbors}.{target_column}.lz4').exists():\n",
    "        continue\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "        samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(n_fold=n_fold))\n",
    "\n",
    "        create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "        samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "        model = knn_classifier(samples, target_column, covariates, n_neighbors=3, metrics='')\n",
    "\n",
    "        model['#_fold'] = n_fold\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    joblib.dump(models, OUTPUT_DIR / f'knn.nn_{n_neighbors}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b576247",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d593c3f",
   "metadata": {},
   "source": [
    "Para treinamento em quantidades maiores de amostras sera importante migrar para abordagens paralelas como propostas pelo framework [cuML SVM](https://medium.com/rapids-ai/fast-support-vector-classification-with-rapids-cuml-6e49f4a7d89e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8da647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(kernel=\"linear\"):\n",
    "    return SVC(kernel=kernel, probability=True, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def linear_svc(samples: pd.DataFrame, target_column: str, covariates: list[str], kernel=\"linear\", sample_ratio=0.01):\n",
    "    x_train = samples.groupby(['tile_id', target_column], group_keys=False).apply(\n",
    "            lambda group: group.sample(frac=sample_ratio, random_state=RANDOM_STATE)\n",
    "        )[covariates]\n",
    "    y_train = samples.loc[x_train.index][target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator(kernel)\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 'training_time': time.time() - t_start}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb76365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = ['linear', 'poly', 'rbf']\n",
    "\n",
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "for kernel in ['linear']:\n",
    "    for target_column in class_name:\n",
    "        if (OUTPUT_DIR / f'svc.k_{kernel}.{target_column}.lz4').exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN.format(n_fold=n_fold))\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = linear_svc(samples, target_column, covariates, kernel)\n",
    "\n",
    "            model['#_fold'] =  n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / f'svc.k_{kernel}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83495a",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdbc4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator():\n",
    "    return LogisticRegression(n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "def random_forest(samples: pd.DataFrame, target_column: str, covariates: list[str]):\n",
    "    x_train = samples[covariates]\n",
    "    y_train = samples[target_column]\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    estimator = get_estimator()\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    return {'model': estimator, 'training_time': time.time() - t_start}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda86e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = ['oxc', 'oxn']\n",
    "class_values = [([0], [1]), ([0], [2])]\n",
    "\n",
    "for kernel in ['linear', 'poly', 'rbf']:\n",
    "    for target_column in class_name:\n",
    "        if (OUTPUT_DIR / f'svc.k_{kernel}.{target_column}.lz4').exists():\n",
    "            continue\n",
    "\n",
    "        models = []\n",
    "\n",
    "        for n_fold in [f'{n:02}' for n in range(1, 6)]:\n",
    "            samples = pd.read_parquet(INPUT_DIR / SAMPLES_FN)\n",
    "\n",
    "            create_ovo_class(samples, class_name, class_values)\n",
    "\n",
    "            samples = samples[np.logical_not(np.isnan(samples[target_column]))]\n",
    "\n",
    "            model = linear_svc(samples, target_column, covariates, kernel)\n",
    "\n",
    "            model['#_fold'] =  n_fold\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "        joblib.dump(models, OUTPUT_DIR / f'svc.k_{kernel}.{target_column}.lz4', compress='lz4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc6250",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c666ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Versão do TensorFlow: {tf.__version__}\")\n",
    "\n",
    "# Lista os dispositivos físicos que o TensorFlow pode usar\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(f\"Número de GPUs encontradas: {len(gpus)}\")\n",
    "    print(f\"Detalhes: {gpus}\")\n",
    "    \n",
    "    try:\n",
    "        # Tenta alocar memória na primeira GPU para confirmar que está funcional\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"✅ GPU está pronta para uso (memory growth ativado).\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"❌ Erro ao inicializar a GPU: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ NENHUMA GPU compatível foi encontrada pelo TensorFlow.\")\n",
    "    print(\"O modelo irá treinar usando a CPU (muito mais lento).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Modelo sequencial com uma única camada densa\n",
    "# melhor modelo com 7 layers 256, activation='relu' e dropout=0.4; early_stopping -> default com 3 espera. Sempre apenas um epoch.\n",
    "model = models.Sequential([\n",
    "    layers.Dense(512, input_shape=(64,), activation='sigmoid'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(512, activation='sigmoid'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(512, activation='sigmoid'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(512, activation='sigmoid'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compila o modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='crossentropy',\n",
    "    metrics=['precision', 'recall']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591524e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_train = pd.read_parquet(INPUT_DIR / 'samples.split_01.frac_20.train.pq')\n",
    "\n",
    "create_ovo_class(samples_train, class_name, class_values)\n",
    "\n",
    "samples_train = samples_train[np.logical_not(np.isnan(samples_train['oxc']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_valid = pd.read_parquet(INPUT_DIR / 'samples.split_01.test.pq')\n",
    "\n",
    "create_ovo_class(samples_valid, class_name, class_values)\n",
    "\n",
    "samples_valid = samples_valid[np.logical_not(np.isnan(samples_valid['oxc']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_precision',     # Métrica a monitorizar\n",
    "    min_delta=0.05,\n",
    "    patience=10,            # Número de épocas sem melhoria antes de parar\n",
    "    verbose=1,              # Imprime uma mensagem quando para\n",
    "    mode='max',             # 'min' porque a perda (loss) deve minimizar\n",
    "    restore_best_weights=True # Restaura os melhores pesos encontrados\n",
    ")\n",
    "\n",
    "model.fit(samples_train[covariates], samples_train['oxc'], epochs=50,\n",
    "            batch_size=8192, callbacks=[early_stopping_monitor],\n",
    "            validation_data=(samples_valid[covariates], samples_valid['oxc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89995269-d616-4da7-beae-b0586467b497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
